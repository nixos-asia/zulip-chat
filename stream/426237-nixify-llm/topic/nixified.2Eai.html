<html>
<head><meta charset="utf-8"><title>nixified.ai · nixify-llm · Zulip Chat Archive</title></head>
<h2>Stream: <a href="http://chat.nixos.asia/stream/426237-nixify-llm/index.html">nixify-llm</a></h2>
<h3>Topic: <a href="http://chat.nixos.asia/stream/426237-nixify-llm/topic/nixified.2Eai.html">nixified.ai</a></h3>

<hr>

<base href="https://nixos.zulipchat.com">

<head><link href="http://chat.nixos.asia/style.css" rel="stylesheet"></head>

<a name="422466540"></a>
<h4><a href="https://nixos.zulipchat.com#narrow/stream/426237-nixify-llm/topic/nixified.ai/near/422466540" class="zl"><img src="http://chat.nixos.asia/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Shivaraj B H <a href="http://chat.nixos.asia/stream/426237-nixify-llm/topic/nixified.2Eai.html#422466540">(Feb 20 2024 at 17:25)</a>:</h4>
<p><a href="https://github.com/nixified-ai/flake">https://github.com/nixified-ai/flake</a></p>
<p>Haven’t tried it yet, but hoping to play around with it this weekend and give me thoughts</p>



<a name="422466715"></a>
<h4><a href="https://nixos.zulipchat.com#narrow/stream/426237-nixify-llm/topic/nixified.ai/near/422466715" class="zl"><img src="http://chat.nixos.asia/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Srid <a href="http://chat.nixos.asia/stream/426237-nixify-llm/topic/nixified.2Eai.html#422466715">(Feb 20 2024 at 17:26)</a>:</h4>
<p>I think <span class="user-mention" data-user-id="678574">@Andreas</span> has a few thoughts on its shortcomings.</p>



<a name="422467630"></a>
<h4><a href="https://nixos.zulipchat.com#narrow/stream/426237-nixify-llm/topic/nixified.ai/near/422467630" class="zl"><img src="http://chat.nixos.asia/assets/img/zulip.svg" alt="view this post on Zulip" style="width:20px;height:20px;"></a> Andreas <a href="http://chat.nixos.asia/stream/426237-nixify-llm/topic/nixified.2Eai.html#422467630">(Feb 20 2024 at 17:30)</a>:</h4>
<p>Well the shortcomings are obviously that you can run two things and not more it seems: InvokeAI and textgen. But what if I want to run Automatic1111 Webui or ComyUI for stable diffusion? And I want to get several LoRAs from <a href="http://civit.ai">civit.ai</a>? Not really a straightforward path.</p>
<p>Like right now I am playing with LLMs using ollama. Which is really nice. But I use the ollama docker images for ROCm because setting up ROCm with StableDiffusionWebUI kinda taught me that the components going into ROCm and their interaction with PyTorch and all the rest of the Python ecosystem might be a bit finicky and you better take a working setup when you find one and never touch it again unless you have a day for experimenting...</p>



<hr><p>Last updated: Mar 01 2026 at 18:33 UTC</p>
</html>