[
    {
        "content": "<p>There was a tool, I forgot the name, which abstracts a wide variety of models behind an openAPI _API_, so that they can be used in its stead from other tooling.</p>\n<p>What was that name?</p>",
        "id": 422468805,
        "sender_full_name": "David Arnold",
        "timestamp": 1708450659
    },
    {
        "content": "<p>It wasn't <a href=\"https://localai.io/\">https://localai.io/</a>, iirc, but it's one such tool.</p>",
        "id": 422470030,
        "sender_full_name": "David Arnold",
        "timestamp": 1708451114
    },
    {
        "content": "<p>ollama also has OpenAI support now</p>",
        "id": 422470702,
        "sender_full_name": "Andreas",
        "timestamp": 1708451384
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"678574\">Andreas</span> <a href=\"#narrow/stream/426237-nixify-llm/topic/Name.20of.20tool.20who.20makes.20all.20LLMV.20OpenAPI.20compatible/near/422470702\">schrieb</a>:</p>\n<blockquote>\n<p>ollama also has OpenAI support now</p>\n</blockquote>\n<p>Maybe it was ollama. But do I misremeber that it could potentially run _any_ model from hugginface?</p>",
        "id": 422471021,
        "sender_full_name": "David Arnold",
        "timestamp": 1708451502
    },
    {
        "content": "<p>ollama pulls in a lot of models from Huggingface. You can take a loot at their library here: <a href=\"https://ollama.com/library\">https://ollama.com/library</a></p>",
        "id": 422471549,
        "sender_full_name": "Andreas",
        "timestamp": 1708451695
    },
    {
        "content": "<p><span class=\"user-mention silent\" data-user-id=\"678574\">Andreas</span> <a href=\"#narrow/stream/426237-nixify-llm/topic/Name.20of.20tool.20who.20makes.20all.20LLMV.20OpenAPI.20compatible/near/422471549\">schrieb</a>:</p>\n<blockquote>\n<p>ollama pulls in a lot of models from Huggingface. You can take a loot at their library here: <a href=\"https://ollama.com/library\">https://ollama.com/library</a></p>\n</blockquote>\n<p>Hm, interesting. Why aren't they consumed from upstream? I just wonder and feel like I miss something.</p>",
        "id": 422471804,
        "sender_full_name": "David Arnold",
        "timestamp": 1708451793
    },
    {
        "content": "<blockquote>\n<p>consumed from upstream?</p>\n</blockquote>\n<p>Which would be from where?</p>",
        "id": 422474546,
        "sender_full_name": "Andreas",
        "timestamp": 1708452728
    },
    {
        "content": "<p>I mean: why do they host models from hugginface on their own site, if that makes sense as a question? Or if it doesn't I'm maybe having the wrong premise.</p>",
        "id": 422474729,
        "sender_full_name": "David Arnold",
        "timestamp": 1708452802
    },
    {
        "content": "<p>Or is it just a matter of having a \"registry\" of compatible models?</p>",
        "id": 422474912,
        "sender_full_name": "David Arnold",
        "timestamp": 1708452851
    },
    {
        "content": "<p>that is a good question to which I have no exact answer right now <span aria-label=\"smiley\" class=\"emoji emoji-1f603\" role=\"img\" title=\"smiley\">:smiley:</span> let me know if you find out.</p>\n<p>However, there is something on the OpenAI compatible API I just found by accident: <a href=\"https://github.com/ollama/ollama/blob/main/docs/openai.md\">https://github.com/ollama/ollama/blob/main/docs/openai.md</a></p>",
        "id": 422475747,
        "sender_full_name": "Andreas",
        "timestamp": 1708453138
    },
    {
        "content": "<p>I dunno, I know perplexities api is openai compatible, which is one of the reasons I decided to pull the trigger on a pro subscription</p>",
        "id": 422476103,
        "sender_full_name": "Tim DeHerrera",
        "timestamp": 1708453272
    },
    {
        "content": "<p>Here's a list: <a href=\"https://kleiber.me/blog/2024/01/07/six-ways-running-llm-locally/\">https://kleiber.me/blog/2024/01/07/six-ways-running-llm-locally/</a></p>\n<p>But I cant recognize the thing I had in mind. Lost knowledge. <span aria-label=\"cry\" class=\"emoji emoji-1f622\" role=\"img\" title=\"cry\">:cry:</span></p>",
        "id": 422476552,
        "sender_full_name": "David Arnold",
        "timestamp": 1708453449
    },
    {
        "content": "<p>The problem is that this space moves so fast that it almost becomes irrelevant if you knew something existed three months ago</p>",
        "id": 422476796,
        "sender_full_name": "Andreas",
        "timestamp": 1708453562
    },
    {
        "content": "<p>Yeah, that is true! I guess from a user perspective (in my case: editor support), it's important to look for the emerging standard api, and choose tools wisely so that they aren't a one-way door. Ideally, one's pick will be the one you can grow within that ecosystem.</p>",
        "id": 422477684,
        "sender_full_name": "David Arnold",
        "timestamp": 1708453914
    },
    {
        "content": "<p>Yeah! Burn them out!</p>\n<p><a href=\"/user_uploads/60244/yzYNcFVHalE4HuJKZLlrW3uD/image.png\">image.png</a></p>\n<div class=\"message_inline_image\"><a href=\"/user_uploads/60244/yzYNcFVHalE4HuJKZLlrW3uD/image.png\" title=\"image.png\"><img src=\"/user_uploads/60244/yzYNcFVHalE4HuJKZLlrW3uD/image.png\"></a></div>",
        "id": 422478601,
        "sender_full_name": "David Arnold",
        "timestamp": 1708454227
    },
    {
        "content": "<p>Where is that one from?</p>",
        "id": 422495820,
        "sender_full_name": "Andreas",
        "timestamp": 1708460906
    },
    {
        "content": "<p>Oh, I don't remeber, but it's emblematic for misaligned expectaions in those ecosystems.</p>",
        "id": 422842620,
        "sender_full_name": "David Arnold",
        "timestamp": 1708616079
    },
    {
        "content": "<p>yes the development of the open source generative A.I. landscape is fast and chaotic right now</p>",
        "id": 422977246,
        "sender_full_name": "Andreas",
        "timestamp": 1708679897
    },
    {
        "content": "<p><a href=\"https://github.com/janhq/nitro\">https://github.com/janhq/nitro</a></p>\n<p>Wasn't it, but seems promising. Small, based on llama.cpp</p>",
        "id": 423117664,
        "sender_full_name": "David Arnold",
        "timestamp": 1708736751
    },
    {
        "content": "<p>Even ROCm support seems to be on it's way! <span aria-label=\"confetti\" class=\"emoji emoji-1f38a\" role=\"img\" title=\"confetti\">:confetti:</span> <a href=\"https://github.com/janhq/nitro/issues/323\">https://github.com/janhq/nitro/issues/323</a> </p>\n<p>I might try it. But it looks more or less like lightweight ollama. However the problem is that in order to run it, I need the 20 GB + something docker image from AMD for ROCm. So the ollama-nitro difference pales in comparison.</p>",
        "id": 423152320,
        "sender_full_name": "Andreas",
        "timestamp": 1708765681
    },
    {
        "content": "<p>This also seems to be taken serious, from a rust perspective, even if shapeshifting quite some, atm: <a href=\"https://github.com/rustformers/llm\">https://github.com/rustformers/llm</a></p>",
        "id": 423152404,
        "sender_full_name": "David Arnold",
        "timestamp": 1708765746
    },
    {
        "content": "<p>Re GGUF, see also this choice of gpt4all:</p>\n<blockquote>\n<p>GPT4All v2.5.0 and newer only supports models in GGUF format (.gguf). Models used with a previous version of GPT4All (.bin extension) will no longer work.</p>\n</blockquote>\n<p>Hints are condensing into knowledge, it appears</p>",
        "id": 423158962,
        "sender_full_name": "David Arnold",
        "timestamp": 1708771075
    }
]